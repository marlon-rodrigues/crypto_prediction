{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.5/dist-packages\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "#For Prediction\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing, cross_validation\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume BTC</th>\n",
       "      <th>Volume USD</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Popularity_Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>6337.9</td>\n",
       "      <td>6984.8</td>\n",
       "      <td>6337.9</td>\n",
       "      <td>6793.5</td>\n",
       "      <td>9855.87</td>\n",
       "      <td>66133073.75</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>6151.4</td>\n",
       "      <td>6430.0</td>\n",
       "      <td>6110.1</td>\n",
       "      <td>6337.9</td>\n",
       "      <td>9872.86</td>\n",
       "      <td>62166515.28</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-09</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>5939.6</td>\n",
       "      <td>6174.0</td>\n",
       "      <td>5933.2</td>\n",
       "      <td>6151.4</td>\n",
       "      <td>7405.18</td>\n",
       "      <td>44816872.39</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>5744.0</td>\n",
       "      <td>5983.2</td>\n",
       "      <td>5660.0</td>\n",
       "      <td>5939.6</td>\n",
       "      <td>6007.11</td>\n",
       "      <td>35184697.52</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>5687.4</td>\n",
       "      <td>5955.8</td>\n",
       "      <td>5687.4</td>\n",
       "      <td>5906.7</td>\n",
       "      <td>3668.12</td>\n",
       "      <td>21496575.65</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Symbol    Open    High     Low   Close  Volume BTC  \\\n",
       "0  2019-05-11  BTCUSD  6337.9  6984.8  6337.9  6793.5     9855.87   \n",
       "1  2019-05-10  BTCUSD  6151.4  6430.0  6110.1  6337.9     9872.86   \n",
       "2  2019-05-09  BTCUSD  5939.6  6174.0  5933.2  6151.4     7405.18   \n",
       "3  2019-05-08  BTCUSD  5744.0  5983.2  5660.0  5939.6     6007.11   \n",
       "4  2019-05-07  BTCUSD  5687.4  5955.8  5687.4  5906.7     3668.12   \n",
       "\n",
       "    Volume USD  Popularity  Popularity_Normalized  \n",
       "0  66133073.75        12.0                 1200.0  \n",
       "1  62166515.28        12.0                 1200.0  \n",
       "2  44816872.39        12.0                 1200.0  \n",
       "3  35184697.52        12.0                 1200.0  \n",
       "4  21496575.65        12.0                 1200.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Datasets \n",
    "research_data_raw = pd.read_csv('Datasets/bitcoin_research_data_v2.csv')\n",
    "# Drop unnecessary columns\n",
    "research_data = research_data_raw.drop('Unnamed: 0', axis=1)\n",
    "research_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume BTC</th>\n",
       "      <th>Volume USD</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Popularity_Normalized</th>\n",
       "      <th>Epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>6337.9</td>\n",
       "      <td>6984.8</td>\n",
       "      <td>6337.9</td>\n",
       "      <td>6793.5</td>\n",
       "      <td>9855.87</td>\n",
       "      <td>66133073.75</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557619199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>6151.4</td>\n",
       "      <td>6430.0</td>\n",
       "      <td>6110.1</td>\n",
       "      <td>6337.9</td>\n",
       "      <td>9872.86</td>\n",
       "      <td>62166515.28</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557532799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-09</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>5939.6</td>\n",
       "      <td>6174.0</td>\n",
       "      <td>5933.2</td>\n",
       "      <td>6151.4</td>\n",
       "      <td>7405.18</td>\n",
       "      <td>44816872.39</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557446399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>5744.0</td>\n",
       "      <td>5983.2</td>\n",
       "      <td>5660.0</td>\n",
       "      <td>5939.6</td>\n",
       "      <td>6007.11</td>\n",
       "      <td>35184697.52</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557359999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>5687.4</td>\n",
       "      <td>5955.8</td>\n",
       "      <td>5687.4</td>\n",
       "      <td>5906.7</td>\n",
       "      <td>3668.12</td>\n",
       "      <td>21496575.65</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557273599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Symbol    Open    High     Low   Close  Volume BTC  \\\n",
       "0  2019-05-11  BTCUSD  6337.9  6984.8  6337.9  6793.5     9855.87   \n",
       "1  2019-05-10  BTCUSD  6151.4  6430.0  6110.1  6337.9     9872.86   \n",
       "2  2019-05-09  BTCUSD  5939.6  6174.0  5933.2  6151.4     7405.18   \n",
       "3  2019-05-08  BTCUSD  5744.0  5983.2  5660.0  5939.6     6007.11   \n",
       "4  2019-05-07  BTCUSD  5687.4  5955.8  5687.4  5906.7     3668.12   \n",
       "\n",
       "    Volume USD  Popularity  Popularity_Normalized       Epoch  \n",
       "0  66133073.75        12.0                 1200.0  1557619199  \n",
       "1  62166515.28        12.0                 1200.0  1557532799  \n",
       "2  44816872.39        12.0                 1200.0  1557446399  \n",
       "3  35184697.52        12.0                 1200.0  1557359999  \n",
       "4  21496575.65        12.0                 1200.0  1557273599  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert data to milliseconds\n",
    "research_data['Epoch'] = pd.to_datetime(research_data['Date'] + ' 23:59:59.000')\n",
    "research_data['Epoch'] = (research_data['Epoch'] - dt.datetime(1970,1,1)).dt.total_seconds().astype('int64')\n",
    "research_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1808\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "research_data_imp2 = research_data.copy()\n",
    "research_data_imp2.sort_values(by='Date', inplace=True, ascending=True) # Sort by date ascending\n",
    "forecast_out = int(math.ceil(0.0001 * len(research_data_imp2))) # 0.01% of data\n",
    "#forecast_out = int(math.ceil(0.0025 * len(research_data_imp2))) # 0.05% of data\n",
    "print(len(research_data_imp2))\n",
    "print(forecast_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume BTC</th>\n",
       "      <th>Volume USD</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Popularity_Normalized</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>5687.4</td>\n",
       "      <td>5955.8</td>\n",
       "      <td>5687.4</td>\n",
       "      <td>5906.7</td>\n",
       "      <td>3668.12</td>\n",
       "      <td>21496575.65</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557273599</td>\n",
       "      <td>5939.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>5744.0</td>\n",
       "      <td>5983.2</td>\n",
       "      <td>5660.0</td>\n",
       "      <td>5939.6</td>\n",
       "      <td>6007.11</td>\n",
       "      <td>35184697.52</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557359999</td>\n",
       "      <td>6151.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-09</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>5939.6</td>\n",
       "      <td>6174.0</td>\n",
       "      <td>5933.2</td>\n",
       "      <td>6151.4</td>\n",
       "      <td>7405.18</td>\n",
       "      <td>44816872.39</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557446399</td>\n",
       "      <td>6337.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>6151.4</td>\n",
       "      <td>6430.0</td>\n",
       "      <td>6110.1</td>\n",
       "      <td>6337.9</td>\n",
       "      <td>9872.86</td>\n",
       "      <td>62166515.28</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557532799</td>\n",
       "      <td>6793.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>BTCUSD</td>\n",
       "      <td>6337.9</td>\n",
       "      <td>6984.8</td>\n",
       "      <td>6337.9</td>\n",
       "      <td>6793.5</td>\n",
       "      <td>9855.87</td>\n",
       "      <td>66133073.75</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1557619199</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Symbol    Open    High     Low   Close  Volume BTC  \\\n",
       "4  2019-05-07  BTCUSD  5687.4  5955.8  5687.4  5906.7     3668.12   \n",
       "3  2019-05-08  BTCUSD  5744.0  5983.2  5660.0  5939.6     6007.11   \n",
       "2  2019-05-09  BTCUSD  5939.6  6174.0  5933.2  6151.4     7405.18   \n",
       "1  2019-05-10  BTCUSD  6151.4  6430.0  6110.1  6337.9     9872.86   \n",
       "0  2019-05-11  BTCUSD  6337.9  6984.8  6337.9  6793.5     9855.87   \n",
       "\n",
       "    Volume USD  Popularity  Popularity_Normalized       Epoch  Prediction  \n",
       "4  21496575.65        12.0                 1200.0  1557273599      5939.6  \n",
       "3  35184697.52        12.0                 1200.0  1557359999      6151.4  \n",
       "2  44816872.39        12.0                 1200.0  1557446399      6337.9  \n",
       "1  62166515.28        12.0                 1200.0  1557532799      6793.5  \n",
       "0  66133073.75        12.0                 1200.0  1557619199         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_data_imp2['Prediction'] = research_data_imp2['Close'].shift(-forecast_out)\n",
    "research_data_imp2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X = np.array(research_data_imp2.drop(['Prediction', 'Symbol', 'Date', 'Popularity'], 1)) # Create X with numeric columns only\n",
    "    # X = np.array(research_data_imp2['Close'])\n",
    "    # X = research_data_imp2['Close'].to_frame()\n",
    "    X = preprocessing.scale(X)\n",
    "    X_forecast = X[-forecast_out:]\n",
    "    X = X[:-forecast_out]\n",
    "\n",
    "    # research_data_imp2.dropna(inplace=True)\n",
    "\n",
    "    y = np.array(research_data_imp2['Prediction'])\n",
    "    y = y[:-forecast_out]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1445, 8)\n",
      "(1445,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer():\n",
    "    return Adam(lr=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 4104      \n",
      "=================================================================\n",
      "Total params: 181,640\n",
      "Trainable params: 181,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_generator():\n",
    "    generator=Sequential()\n",
    "    generator.add(Dense(units=128,input_dim=100))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(units=256))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(units=512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    generator.add(Dense(units=8, activation='tanh'))\n",
    "    \n",
    "    #generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer(), metrics=['accuracy'])\n",
    "    generator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return generator\n",
    "g=create_generator()\n",
    "g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 168,961\n",
      "Trainable params: 168,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_discriminator():\n",
    "    optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "\n",
    "    discriminator=Sequential()\n",
    "    discriminator.add(Dense(units=512,input_dim=8))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "       \n",
    "    \n",
    "    discriminator.add(Dense(units=256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "       \n",
    "    discriminator.add(Dense(units=128))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    \n",
    "    discriminator.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return discriminator\n",
    "d =create_discriminator()\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 8)                 181640    \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    (None, 1)                 168961    \n",
      "=================================================================\n",
      "Total params: 350,601\n",
      "Trainable params: 181,640\n",
      "Non-trainable params: 168,961\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_gan(discriminator, generator):\n",
    "    optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "    discriminator.trainable=False\n",
    "    gan_input = Input(shape=(100,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output= discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return gan\n",
    "gan = create_gan(d,g)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_1 (Sequential)    (None, 8)                 181640    \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    (None, 1)                 168961    \n",
      "=================================================================\n",
      "Total params: 350,601\n",
      "Trainable params: 181,640\n",
      "Non-trainable params: 168,961\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_gan2(d, g):\n",
    "    optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "    AM = Sequential()\n",
    "    AM.add(g)\n",
    "    AM.add(d)\n",
    "    AM.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "        metrics=['accuracy'])\n",
    "    return AM\n",
    "gan2 = create_gan2(d,g)\n",
    "gan2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(epoch, generator, examples=100):\n",
    "    (X_train, y_train, X_test, y_test) = load_data()\n",
    "    noise= np.random.normal(loc=0, scale=1, size=[examples, 100])\n",
    "    generated_prices = generator.predict(noise)\n",
    "    \n",
    "    predictions = np.array([p['predictions'][0] for p in generated_prices])\n",
    "\n",
    "    #print(generated_prices)\n",
    "    \n",
    "    # print(\"The Explained Variance: %.2f\" % generator.score(X_test, y_test))  \n",
    "    print(\"The Explained Variance: %.2f\" % metrics.explained_variance_score(y_test, predictions))\n",
    "    print(\"The Mean Absolute Error: %.2f\" % metrics.mean_absolute_error(y_test, predictions))  \n",
    "    print(\"The Median Absolute Error: %.2f\" % metrics.median_absolute_error(y_test, predictions)) \n",
    "    print(\"The Mean Squared Error: %.2f\" % metrics.mean_squared_error(y_test, predictions)) \n",
    "    print(\"The Root Mean Squared Error: %.2f\" % (np.sqrt(metrics.mean_squared_error(y_test, predictions))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epochs=1, batch_size=128):\n",
    "    #Loading the data\n",
    "    (X_train, y_train, X_test, y_test) = load_data()\n",
    "    batch_count = X_train.shape[0] / batch_size\n",
    "    \n",
    "    # Creating GAN\n",
    "    generator = create_generator()\n",
    "    discriminator = create_discriminator()\n",
    "    gan = create_gan(discriminator, generator)\n",
    "    #gan = create_gan2(discriminator, generator)\n",
    "    \n",
    "    for e in range(1, epochs+1 ):\n",
    "        print(\"Epoch %d\" %e)\n",
    "        for i in tqdm(range(batch_size)):\n",
    "            #generate  random noise as an input  to  initialize the  generator\n",
    "            noise = np.random.normal(0,1, [batch_size, 100])\n",
    "            \n",
    "            # Generate fake prices from noised input\n",
    "            generated_prices = generator.predict(noise)\n",
    "            \n",
    "            # Get a random set of real prices\n",
    "            prices_batch = X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]\n",
    "            \n",
    "            #Construct different batches of real and fake data \n",
    "            X = np.concatenate([prices_batch, generated_prices])\n",
    "            \n",
    "            # Labels for generated and real data\n",
    "            y_dis = np.zeros(2*batch_size)\n",
    "            y_dis[:batch_size]=0.9\n",
    "            \n",
    "            #Pre train discriminator on fake and real data before starting the gan. \n",
    "            discriminator.trainable = True\n",
    "            d_loss = discriminator.train_on_batch(X, y_dis)\n",
    "            \n",
    "            #Tricking the noised input of the Generator as real data\n",
    "            noise = np.random.normal(0,1, [batch_size, 100])\n",
    "            y_gen = np.ones(batch_size)\n",
    "            \n",
    "            # During the training of gan, \n",
    "            # the weights of discriminator should be fixed. \n",
    "            #We can enforce that by setting the trainable flag\n",
    "            discriminator.trainable=False\n",
    "            \n",
    "            #training  the GAN by alternating the training of the Discriminator \n",
    "            #and training the chained GAN model with Discriminator’s weights freezed.\n",
    "            a_loss = gan.train_on_batch(noise, y_gen)\n",
    "            \n",
    "            if i == (batch_size-1):\n",
    "                log_mesg = \"%d: [D loss: %f, acc: %f]\" % (e, d_loss[0], d_loss[1])\n",
    "                log_mesg = \"%s  [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "                print(log_mesg)\n",
    "            \n",
    "        #if e == 1 or e % 20 == 0:\n",
    "        #    get_accuracy(e, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "  0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:07<00:00, 17.26it/s]\n",
      "  2%|▏         | 3/128 [00:00<00:05, 21.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: [D loss: 0.576112]  [A loss: 1.046994]\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:05<00:00, 26.29it/s]\n",
      "  2%|▏         | 3/128 [00:00<00:05, 21.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [D loss: 0.567893]  [A loss: 1.122433]\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:05<00:00, 25.25it/s]\n",
      "  2%|▏         | 3/128 [00:00<00:05, 24.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: [D loss: 0.546935]  [A loss: 1.233090]\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:05<00:00, 24.55it/s]\n",
      "  2%|▏         | 3/128 [00:00<00:05, 21.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: [D loss: 0.494721]  [A loss: 1.329924]\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 29/128 [00:01<00:04, 23.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2aa34948f273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#training(400,128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-2b7e6f0e11ac>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epochs, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#Pre train discriminator on fake and real data before starting the gan.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#Tricking the noised input of the Generator as real data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[1;32m   1066\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                                          class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training(400,128)\n",
    "training(5,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
